---
title: "Tree models in ChEMBL data"
subtitle: "ML series 2.1 - Decision tree - data pre-processing and preparation"
draft: true
jupyter: python3
format: html
---

#### **Introduction**

I've now come to a stage to do some more in-depth machine learning work after reading some peer-reviewed papers about it in relation to drug discovery and cheminformatics. Previously, I've only lightly touched on a commonly used classifier algorithm, logistic regression, as the first series in the machine learning realm. Reflecting back, I think I could've done a more thorough job during data preparation stage. So this would be attempted this time.

From a few of the papers I've read so far, dated in the recent years of 2021 and 2022, it seemed that traditional machine learning (ML) methods were still indispensible performance-wise, and along with deep learning neural networks, they tend to increase prediction accuracy a bit more. I haven't ventured into the practicality and usefulness of large language models in drug discovery yet. From what I could gather from experienced seniors in this area, they were still very much too novel to be useful, and from what I could imagine, molecular representations in texts or strings probably had already created numerous headaches e.g. standardisations regarding to whether to use canonical SMILES and/or SELFIES or not, and probably compound chiralities and so on. Because of this, I'd be sticking with learning to walk first in the conventional ML area before trying to jog and run actually (plans to work on deep learning in the future).

The data preparation was carried out with strong reference to the materials and methods section in this paper: van Tilborg, D. *et al*. J. Chem. Inf.Model. 2022, 62, 5938-5951. There were probably other methods out there, but this was the paper I've read recently that had made sense and relatively easy to follow.

<br>

#### **Data retrieval**

This time I've decided to try something new which was to use the ChEMBL webresource client to get data from the scratch (i.e. not from direct file downloads from the ChEMBL website). I found this great online resource about fetching data this way here - reference: TeachOpenCADD talktorial on [compound data acquisition(https://projects.volkamerlab.org/teachopencadd/talktorials/T001_query_chembl.html)]. The data retrieval workflow used below was mainly adapted from this cited talktorial.

The webresource client was supported by ChEMBL group and based on a Django QuerySet interface - their [GitHub repository](https://github.com/chembl/chembl_webresource_client) might explain a bit more about it, particularly the Jupyter notebook provided in the repository would help a lot for how to write code to search for specific data.

To do this, a few libraries were needed first.

```{python}
# Import libraries
# Fetch data through ChEMBL webresource client
from chembl_webresource_client.new_client import new_client

# RDKit modules
#from rdkit.Chem import PandasTools

# Dataframe library
#import numpy as np
import pandas as pd

# Progress bar
from tqdm import tqdm
```

To see what types of data were provided by ChEMBL webresource client, run the following code and refer to ChEMBL documentations to find out what data were embedded inside which data categories. Sometimes, it might not be that straight forward and some digging would be required (I went back to this step to find the "data_validity_comment" when I got to the compound sanitisation step actually).

```{python}
available_resources = [resource for resource in dir(new_client) if not resource.startswith('_')]
print(available_resources)
```

Create resource objects to enable API access as suggested.

```{python}
# for targets (proteins)
targets_api = new_client.target

# for bioactivities
bioact_api = new_client.activity

# for compounds
cpd_api = new_client.molecule
```

Check object type for one of these API objects (e.g. bioactivity API object).

```{python}
type(bioact_api)
```

<br>

##### **Fetching target data**

Select a protein target e.g. acetylcholinesterase (this was randomly chosen).

```{python}
# Specify Uniprot ID for acetylcholinesterase
uniprot_id = "P22303"

# Get info from ChEMBL about this protein target, 
# with selected features only
targets = targets_api.get(target_components__accession = uniprot_id).only(
    "target_chembl_id",
    "organism", 
    "pref_name", 
    "target_type"
)
```

The query results were stored in a "targets" object, which was a QuerySet with lazy data evaluation only, meaning it would only react when there was a request for the data.

```{python}
# Read "targets" with Pandas
targets = pd.DataFrame.from_records(targets)
targets
```

Select protein target from this dataframe - choosing the first one.

```{python}
# Save the first protein in the dataframe
select_target = targets.iloc[0]
select_target
```

Save the selected ChEMBL ID first (to be used later).

```{python}
chembl_id = select_target.target_chembl_id
# Check it's saved
print(chembl_id)
```

<br>

##### **Fetching bioactivity data**

Obtaining bioactivity data for the selected target.

```{python}
bioact = bioact_api.filter(
    # Use the previously saved target ChEMBL ID
    target_chembl_id = chembl_id, 
    # Bioactivity type
    type = "IC50",
    # Requesting exact measurements
    relation = "=",
    # Binding data as "B"
    assay_type = "B"
).only(
    "activity_id",
    "data_validity_comment"
    "assay_chembl_id",
    "assay_description",
    "assay_type",
    "molecule_chembl_id",
    "type",
    "standard_units",
    "relation",
    "standard_value",
    "target_chembl_id",
    "target_organism",
)

# Check the length and type of bioactivities object
print(len(bioact), type(bioact))
```

To have a quick look at the data being held inside each entry of the bioact dataset, e.g. for first entry.

```{python}
print(len(bioact[0]), type(bioact[0]))
bioact[0]
```

The next step might take a few minutes - downloading the QuerySet as a Pandas DataFrame.

```{python}
bioact_df = pd.DataFrame.from_dict(bioact)

bioact_df.head()
```

Check total rows and columns in the bioactivities dataframe.

```{python}
bioact_df.shape
```

<br>

###### **Pre-process bioactivity data**

To see a variety of units being recorded in the ChEMBL database - would need to be converted to nM. 

```{python}
bioact_df["units"].unique()
```

The simplest thing to do was to drop the "units" and "value" columns, which were in micromolar (*m*M), and to avoid unit conversion to nanomolar (nM), we could use columns such as "standard_units" and "standard_value" which were recorded with nM already.

```{python}
bioact_df.drop(["units", "value"], axis = 1, inplace = True)
# Re-check df
bioact_df.head()
```

```{python}
bioact_df.dtypes
```

```{python}
# Small detour test
# Tried with Polars - standard_value casted as "str", so would still need to convert to floats
# import polars as pl
# ba_df = pl.from_pandas(bioact_df)
# ba_df.head()
```

Convert the column of "standard_value" from object to floats - so we could use this half maximal inhibitory concentration (IC50) values for calculations later.

```{python}
bioact_df = bioact_df.astype({"standard_value": "float64"})
# Check column data types again
bioact_df.dtypes
```

Taking care of any missing entries by removing them in the first place. I excluded "data_validity_comment" column in this exercise as this was required to check if there were any activity data of the compounds that were deemed to be not as valid e.g. excessively high IC50 value and so on. A lot of the compounds in this column probably had "None", which ensured that there were no particular alarm bells to the curated bioactivity data.

```{python}
bioact_df.dropna(subset = ["activity_id", "assay_chembl_id", "assay_description", "assay_type", "molecule_chembl_id", "relation", "standard_units", "standard_value", "target_chembl_id", "target_organism", "type"], axis = 0, how = "any", inplace = True)
# Check number of rows and columns again (in this case, there appeared to be no change)
bioact_df.shape
```

Since all unique units inside the "units" and "values" columns were checked previously, I'd done the same for the "standard_units" column to see the ones recorded in it.

```{python}
bioact_df["standard_units"].unique()
```

There were a mixture of nM, mM and also picomolars (pM)! 

```{python}
# Check for number of non-nM units
bioact_df[bioact_df["standard_units"] != "nM"].shape[0]
```

There appeared to be 30 non-nM values inside the fetched bioactivity data.

```{python}
bioact_df = bioact_df[bioact_df["standard_units"] == "nM"]
```

Then narrow the results to only "nM" and check the dataframe again to see what units were left now.

```{python}
# Check there were only nM
bioact_df["standard_units"].unique()
```

So the filtering worked and currently the number of rows and columns had been reduced.

```{python}
# Check df rows & columns
bioact_df.shape
```

Next part would be to remove all duplicates in the dataframe, especially when there were duplicate tests for the same compound.

```{python}
bioact_df.drop_duplicates("molecule_chembl_id", keep = "first", inplace = True)
```

Renamed the "standard_value" and "standard_units" columns to be "IC50" and "units" respectively.

```{python}
bioact_df.rename(
    columns = {
        "standard_value": "IC50",
        "standard_units": "units"
    }, inplace = True
)

# Check df to ensure name change
bioact_df.head()
```

Lastly, the index of the dataframe was reset.

```{python}
bioact_df.reset_index(drop = True, inplace = True)
bioact_df.head()
```

One final check on the number of columns and rows after pre-processing the bioactivity dataframe.

```{python}
bioact_df.shape
```

There were a total of 11 columns with 5,041 rows of data left in the bioactivity dataframe.

<br>

##### **Fetching compound data**

While having identified the protein target and obtained the bioactivity data, this step was to link the bioactivity data to the compounds.

This could be done by having the ChEMBL IDs available for the compounds involved in the bioactivity dataset.

```{python}
cpds = cpd_api.filter(
    molecule_chembl_id__in = list(bioact_df["molecule_chembl_id"])
).only(
    "molecule_chembl_id",
    "molecule_structures"
)
```

Here, the same step was applied where the compound QuerySet object was converted into a Pandas dataframe. However, the dataset extracted this time could take longer than the time taken to fetch the bioactivity data. This was then monitored using a progress bar (tqdm package).

```{python}
compds = list(tqdm(cpds))
```

Converting retrieved compound QuerySet into a Pandas DataFrame.

```{python}
cpds_df = pd.DataFrame.from_records(compds)

print(cpds_df.shape)
cpds_df.head()
```

<br>

###### **Pre-process compound data**

Removing any missing entries in the compound data.

```{python}
cpds_df.dropna(axis = 0, how = "any", inplace = True)

# Check columns & rows in df
cpds_df.shape
```

Removing any duplicates in the compound data.


```{python}
cpds_df.drop_duplicates("molecule_chembl_id", keep = "first", inplace = True)

# Check columns & rows again
cpds_df.shape
```

Ideally, only the compounds with canonical SMILES would be kept. Checking for the types of molecular representations used in the "molecule_structures" column of the compound dataset.

```{python}
# Randomly choosing the 2nd entry
cpds_df.iloc[1].molecule_structures.keys()
```

There were 4 types as: "canonical_smiles", "molfile", "standard_inchi" and "standard_inchi_key".


```{python}
# Create an empty list to store the canonical smiles
can_smiles = []

# Create a for loop to loop over each row of data, 
# searching for only canonical_smiles to append to the created list
for i, cpd in cpds_df.iterrows():
    try:
        can_smiles.append(cpd["molecule_structures"]["canonical_smiles"])
    except KeyError:
        can_smiles.append(None)

# Create a new df column with name as "smiles", 
# which will store all the canonical smiles collected from the list above
cpds_df["smiles"] = can_smiles
```

Check the compound dataframe quickly to see if a new column for SMILES has been created.

```{python}
cpds_df.head()
```

Once confirmed, the old "molecule_structures" column was then removed.

```{python}
cpds_df.drop("molecule_structures", axis = 1, inplace = True)
```

Finally, adding another step to ensure all missing entries or entries without canonical SMILES strings were removed from the compound dataset.

```{python}
cpds_df.dropna(axis = 0, how = "any", inplace = True)

print(cpds_df.shape)
```

Final look at the compound dataset, which should only include compounds ChEMBL IDs and SMILES columns.

```{python}
cpds_df.head()
```

<br>

##### **Combining bioactivity and compound data**

To combine both datasets, the key was to look for common column (similar to a SQL "join" query) between the two datasets. 

Listing all the column names for both datasets would show the common column.

```{python}
bioact_df.columns
```

```{python}
cpds_df.columns
```

Clearly, the one column that existed in both dataframes was the "molecule_chembl_id" column.

The next step was to combine or merge both datasets.

```{python}
# Create a final dataframe that will contain both bioactivity and compound data
dtree_df = pd.merge(
    bioact_df[["molecule_chembl_id", "IC50", "units", "data_validity_comment"]],
    cpds_df,
    on = "molecule_chembl_id",
)

dtree_df.head()
```

Row indices were reset and shape of the final dataframe was checked.

```{python}
dtree_df.reset_index(drop = True, inplace = True)

print(dtree_df.shape)
```

Saving a copy of the merged dataframe for now (to avoid re-running the previous code repeatedly).

```{python}
dtree_df.to_csv("ache_chembl.csv")
```

<br>

##### **Re-import saved dataframe**

Re-import partly pre-processed dtree_df.

```{python}
dtree_df = pd.read_csv("ache_chembl.csv")
dtree_df.head()
```

Noticed there was an extra index column, likely inherited from how the .csv file was saved, which was subsequently removed.

```{python}
dtree_df = dtree_df.drop("Unnamed: 0", axis = 1)
dtree_df.head()
```

The merged dataframe was found to have IC50 with zero nM, which meant the function to convert IC50 to pIC50 would not proceed (due to natural log of zero normally means undefined answer!). So a good practice to clean data like this was probably best to run a statistical summary such as the code below first, then look for minimum and maximum values and also others to see if there were anything to be tidied up first.

```{python}
dtree_df.describe()
```

Limiting the IC50 values to be above zero only.

```{python}
# Select IC50 values above zero
dtree_df = dtree_df[dtree_df["IC50"] > 0.0]
```

Re-checked the minimum value of IC50 column, which should be above zero.

```{python}
dtree_df["IC50"].min()
```

Now we could convert the IC50 values to pIC50 values (the negative log of IC50 in molar units).

The key to understand pIC50 here was to treat pIC50 similarly to how we understand pH for our acids and bases. pIC50 was a dimensionless value (so no units actually!) - *useful link*. The formula to convert IC50 to pIC50 for nM units was (**use LaTex for formula**):

pIC50 = 9 - log10(IC50)

Import the math library first and set up a small function to do the conversion.

```{python}
import math

def calc_pIC50(IC50):
    pIC50_value = 9 - math.log10(IC50)
    return pIC50_value
```

Applying the calc_pIC50 function to convert all rows of the compound dataset for the IC50 column.

```{python}
# Create a new column for pIC50
# Apply calc_pIC50 function to the data in IC50 column
dtree_df["pIC50"] = dtree_df.apply(lambda x: calc_pIC50(x.IC50), axis = 1)
```

The dataframe would now look like this, with a new pIC50 column ready for use.

```{python}
dtree_df.head()
```

However, for a decision tree model, a few more molecular descriptors were probably needed rather than only IC50 or pIC50 and SMILES... One way to do this could be through computations based on canonical SMILES of compounds by using RDKit to add some molecular descriptors.

Before doing this, a compound sanitisation step would probably be the best before starting any calculations, as this might rule out some compounds with questionable chemical validities. This could also be done via RDKit or I guess Datamol (a Python wrapper library built based on RDKit) might also help as well.

```{python}
#df['column'].astype('string') 
dtree_df = dtree_df.astype({"smiles": "string", "data_validity_comment": "string"})
dtree_df.dtypes
```

Before I jumped straight to compound sanitisation, I thought I should check out if there were any variations in the "data_validity_comment" column.

```{python}
dtree_df["data_validity_comment"].unique()
```

Interestingly, there were 3 different types of data validity comments found, which were "NaN", "Outside typical range" and "Potential transcirption error". So, this meant we would need to address compounds with comments in the latter two.

```{python}
# Find out number of compounds with "outside typical range" as data validity comment
dtree_df[dtree_df["data_validity_comment"] == "Outside typical range"]
```

There were a total of 328 compounds with IC50 outside typical range!

```{python}
# Find out number of compounds with "potential transcription error" as data validity comment
dtree_df[dtree_df["data_validity_comment"] == "Potential transcription error"]
```

There were 8 compounds with potential transcription errors for their respective IC50 values!

This meant it would be best to remove above compounds with questionable IC50 values (could be potential sources of errors for ML models later on). One of the ways I thought of doing was to fill the empty cells under "data_validity_comment" column, so this would be easier to filter.

```{python}
# Fill "NaN" entries with an actual name e.g. zero
dtree_df = dtree_df.fillna("zero")
dtree_df.head(10)
```

Filtered out only the compounds with nil data validity comments.

```{python}
#dtree_df["data_validity_comment"].unique()
dtree_df = dtree_df[dtree_df["data_validity_comment"] == "zero"]
```

Checking the dtree_df dataframe again and also whether if only the compounds with "zero" labelled for "data_validity_comment" column were kept (and other two types were removed).

```{python}
print(dtree_df.shape)
dtree_df["data_validity_comment"].unique()
```

<br>

#### **Compound sanitisation**

I've found the [pre-processing molecules tutorial](https://docs.datamol.io/stable/tutorials/Preprocessing.html) and its reference links provided provided by Datamol at the bottom of the webpage to be very informative. Each steps of fix_mol(), sanitize_mol() and standardize_mol() were explained to a certain degree in the link provided above. I guess the key was to select preprocessing options required to fit the purpose of the ML model being built later on, and experiences would also help to improve the compound preprocessing step.

```{python}
import datamol as dm

# _preprocess function to sanitise compounds - adapted from datamol.io

smiles_column = "smiles"

dm.disable_rdkit_log()

def _preprocess(row):
    # Convert each compound to a RDKit molecule in the smiles column
    mol = dm.to_mol(row[smiles_column], ordered=True)
    # Fix common errors in the molecules
    mol = dm.fix_mol(mol)
    # Sanitise the molecules 
    mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)
    # Standardise the molecules
    mol = dm.standardize_mol(
        mol,
        disconnect_metals=False,
        normalize=True,
        reionize=True,
        # Decided to switch on "uncharge" to neutralise charges - the only change
        uncharge=True,
        stereo=True,
    )

    # Added a new column below for RDKit molecules
    row["rdkit_mol"] = dm.to_mol(mol)
    row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
    row["selfies"] = dm.to_selfies(mol)
    row["inchi"] = dm.to_inchi(mol)
    row["inchikey"] = dm.to_inchikey(mol)
    return row
```

Then the compound sanitisation function was applied to the dtree_df. 

```{python}
dtree_san_df = dtree_df.apply(_preprocess, axis = 1)
dtree_san_df.head()
```

Please note if the dataset required for sanitisation is large, Datamol has suggested using their example code to add parallelisation as shown below.

```{{python}}
# Code adapted from: https://docs.datamol.io/stable/tutorials/Preprocessing.html#references
data_clean = dm.parallelized(
    _preprocess, 
    data.iterrows(), 
    arg_type="args", 
    progress=True, 
    total=len(data)
    )
data_clean = pd.DataFrame(data_clean)
```

```{python}
dtree_san_df.shape
```

In this case, I tried using the preprocessing function without adding the parallelisation, the whole sanitisation process wasn't overly long, and was done within a minute or so (the dtree_df dataframe had 4,703 rows or compounds only).

<br>

#### **Detect outliers**

Plotting a histogram to see the distribution of pIC50 values first.

```{python}
dtree_san_df.hist(column = "pIC50")
```

I read a bit about Dixon's Q test and realised that there were a few required assumptions prior to using this test, and the current dataset being used here (dtree_san_df) might not fit the requirements, which were: 

- normally distributed data 
- a small sample size i.e. e.g. between 3 and 10 (as originally stated in the paper published by R. B. Dean and W. J. Dixon (1951) Simplified Statistics for Small Numbers of Observations”. Anal. Chem., 1951, 23 (4), 636–638).

So I've decided that rather than showing Python code for Dixon's Q test myself, I'd attach a few examples from others instead, [Q test from Plotly](https://plotly.com/python/v3/outlier-test/) and also [Dixon's Q test for outlier identification – a questionable practice](https://sebastianraschka.com/Articles/2014_dixon_test.html), since this dataset here wasn't quite normally distributed as shown from the histogram plotted above. 

```{python}
dtree_san_df.boxplot(column = "pIC50")

# the boxplot version below shows a blank background
# rather than above version with horizontal grid lines
#dtree_san_df.plot.box(column = "pIC50")
```

So I used Pandas' built-in boxplot in addition to the histogram to show the likely outliers within the pIC50 values. Clearly, the possible outliers for pIC50 values appeared to be close to 10 and above. I've then decided not to completely remove these outliers due to the dataset itself not in a Gaussian distribution (as they might not be true outliers).

<br>

#### **Molecular descriptors**