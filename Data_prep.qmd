---
title: "Tree models in ChEMBL data"
subtitle: "ML series 2.1 - Decision tree"
draft: true
jupyter: python3
format: html
---

#### **Introduction**

I've now come to a stage to do some more in-depth machine learning work after reading some peer-reviewed papers about it in relation to drug discovery and cheminformatics. Previously, I've only lightly touched on a commonly used classifier algorithm, logistic regression, as the first series in the machine learning realm. Reflecting back, I think I could've done a more thorough job during data preparation stage. So this would be attempted this time.

From a few of the papers I've read so far, dated in the recent years of 2021 and 2022, it seemed that traditional machine learning (ML) methods were still indispensible performance-wise, and along with deep learning neural networks, they tend to increase prediction accuracy a bit more. I haven't ventured into the practicality and usefulness of large language models in drug discovery yet. From what I could gather from experienced seniors in this area, they were still very much too novel to be useful, and from what I could imagine, molecular representations in texts or strings probably had already created numerous headaches e.g. standardisations regarding to whether to use canonical SMILES and/or SELFIES or not, and probably compound chiralities and so on. Because of this, I'd be sticking with learning to walk first in the conventional ML area before trying to jog and run actually (plans to work on deep learning in the future).

The data preparation was carried out with strong reference to the materials and methods section in this paper: van Tilborg, D. *et al*. J. Chem. Inf.Model. 2022, 62, 5938-5951. There were probably other methods out there, but this was the paper I've read recently that had made sense and relatively easy to follow.

<br>

#### **Data retrieval**

This time I've decided to try something new which was to use the ChEMBL webresource client to get data from the scratch (i.e. not from direct file downloads from the ChEMBL website). I found this great online resource about fetching data this way here - reference: TeachOpenCADD talktorial on [compound data acquisition(https://projects.volkamerlab.org/teachopencadd/talktorials/T001_query_chembl.html)]. The data retrieval workflow used below was mainly adapted from this cited talktorial.

The webresource client was supported by ChEMBL group and based on a Django QuerySet interface - their [GitHub repository](https://github.com/chembl/chembl_webresource_client) might explain a bit more about it.

To do this, a few libraries were needed first.

```{python}
# Import libraries
# Fetch data through ChEMBL webresource client
from chembl_webresource_client.new_client import new_client

# RDKit modules
from rdkit.Chem import PandasTools

# Dataframe library
#import numpy as np
import pandas as pd

# Progress bar
from tqdm import tqdm
```

Create resource objects to enable API access as suggested.

```{python}
# for targets
targets_api = new_client.target

# for compounds
cpd_api = new_client.molecule

# for bioactivities
bioact_api = new_client.activity
```

Check object type for one of these API objects (e.g. bioactivity API object).

```{python}
type(bioact_api)
```

<br>

##### **Fetching target data**

Select a protein target e.g. acetylcholinesterase (this was randomly chosen).

```{python}
# Specify Uniprot ID for acetylcholinesterase
uniprot_id = "P22303"

# Get info from ChEMBL about this protein target, 
# with selected features only
targets = targets_api.get(target_components__accession = uniprot_id).only(
    "target_chembl_id",
    "organism", 
    "pref_name", 
    "target_type"
)
```

The query results were stored in a "targets" object, which was a QuerySet with lazy data evaluation only, meaning it would only react when there was a request for the data.

```{python}
# Read "targets" with Pandas
targets = pd.DataFrame.from_records(targets)
targets
```

Select protein target from this dataframe - choosing the first one.

```{python}
# Save the first protein in the dataframe
select_target = targets.iloc[0]
select_target
```

Save the selected ChEMBL ID first (to be used later).

```{python}
chembl_id = select_target.target_chembl_id
# Check it's saved
print(chembl_id)
```

<br>

##### **Fetching bioactivity data**

Obtaining bioactivity data for the selected target.

```{python}
bioact = bioact_api.filter(
    # Use the previously saved target ChEMBL ID
    target_chembl_id = chembl_id, 
    # Bioactivity type
    type = "IC50",
    # Requesting exact measurements
    relation = "=",
    # Binding data as "B"
    assay_type = "B"
).only(
    "activity_id",
    "assay_chembl_id",
    "assay_description",
    "type",
    "standard_units",
    "relation",
    "standard_value",
    "target_chembl_id",
    "target_organism",
)

# Check the length and type of bioactivities object
print(len(bioact), type(bioact))
```

To have a quick look at the data being held inside each entry of the bioact dataset, e.g. for first entry.

```{python}
print(len(bioact[0]), type(bioact[0]))
bioact[0]
```

The next step might take a few minutes - downloading the QuerySet as a Pandas dataframe.

```{python}
bioact_df = pd.DataFrame.from_dict(bioact)

bioact_df.head()
```

Check total rows and columns in the bioactivities dataframe.

```{python}
print(bioact_df.shape)
```

<br>

##### **Pre-process bioactivity data**

TODO:

*Dropping "units" & "value" columns (as these were in microM - avoids unit conversion to nanoM, which were already present, named "standard_units" & "standard_value")

*?convert bioact_df to Polars df and check column data types (potentially may be more accurate?)

<br>

##### **Fetching compound data**