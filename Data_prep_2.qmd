---
title: "Decision tree"
subtitle: "ML series 2.1 - data pre-processing (part 2)"
author: Jennifer HY Lin
date: '2023-9-8'
draft: true
categories: 
    - Machine learning projects
    - Pandas
    - ChEMBL database
jupyter: python3
format: html
bibliography: references.bib
---

##### **Data source**

The data used in part 2 of data pre-processing was extracted from ChEMBL database by using ChEMBL webresource client in Python. The details of all the steps taken to reach the final .csv file could be seen in this earlier *post*. Since data preparation and cleaning was a well-known process that would take up a lot of time before doing anything significant on the data, I've splitted this process into two posts to ensure reasonable reading time for each.

```{python}
# Import all libraries used
import pandas as pd
import math
from rdkit.Chem import Descriptors
import datamol as dm
# tqdm library used in datamol's batch descriptor code
from tqdm import tqdm
```

<br>

##### **Re-import saved data**

Re-imported the partly pre-processed data from the earlier post.

```{python}
dtree_df = pd.read_csv("ache_chembl.csv")
dtree_df.head()
```

Noticed there was an extra index column (named "Unnamed: 0"), likely inherited from how the .csv file was saved, dropping this column for now.

```{python}
dtree_df = dtree_df.drop("Unnamed: 0", axis = 1)
dtree_df.head()
```

<br>

##### **Calculate pKi**

The distribution of Ki values were shown below via a simple statistical summary.

```{python}
dtree_df.describe()
```

From the above quick statistical summary and also the code below to find the minimum Ki value, it confirmed that there were no zero Ki values recorded.

```{python}
dtree_df["Ki"].min()
```

Now we could convert the Ki values to pKi values, which were the negative logs of Ki in molar units (a [PubChem example](https://pubchem.ncbi.nlm.nih.gov/bioassay/213088) might help to explain it a little further). The key to understand pKi here was to treat pKi similarly to how we would normally understand pH for our acids and bases. The formula to convert Ki to pKi for nanomolar (nM) units was: 

$$
\text{pKi} = 9 - log _{10}(Ki)
$$

Set up a small function to do the conversion.

```{python}
def calc_pKi(Ki):
    pKi_value = 9 - math.log10(Ki)
    return pKi_value
```

Applying the calc_pKi function to convert all rows of the compound dataset for the "Ki" column (please scroll to the very right to see it).

```{python}
# Create a new column for pKi
# Apply calc_pKi function to the data in Ki column
dtree_df["pKi"] = dtree_df.apply(lambda x: calc_pKi(x.Ki), axis = 1)
```

The dataframe would now look like this, with a new pKi column.

```{python}
dtree_df.head()
```

<br>

##### **Plan other data pre-processing steps**

However, for a decision tree model, a few more molecular descriptors were most likely needed rather than only Ki or pKi and SMILES. One way to do this could be through computations based on canonical SMILES of compounds by using RDKit to add some molecular descriptors.

However, before doing this, a compound sanitisation step should be applied before starting any calculations to rule out compounds with questionable chemical validities. RDKit or Datamol (a Python wrapper library built based on RDKit) was also capable of doing this.

I've added a quick step here to convert the data types of "smiles" and "data_validity_comment" columns to string (in case of running into problems later).

```{python}
dtree_df = dtree_df.astype({"smiles": "string", "data_validity_comment": "string"})
dtree_df.dtypes
```

<br>

##### **Check data validity**

Also, before jumping straight to compound sanitisation, I remembered that I needed to check the "data_validity_comment" column.

```{python}
dtree_df["data_validity_comment"].unique()
```

There were 3 different types of data validity comments found, which were "NaN", "Outside typical range" and "Potential transcirption error". So, this meant compounds with comments other than "NaN" would need to be addressed first.

```{python}
# Find out number of compounds with "outside typical range" as data validity comment
dtree_df_err = dtree_df[dtree_df["data_validity_comment"] == "Outside typical range"]
print(dtree_df_err.shape)
dtree_df_err.head()
```

There were a total of 58 compounds with Ki outside typical range.

```{python}
# Find out number of compounds with "potential transcription error" as data validity comment
dtree_df[dtree_df["data_validity_comment"] == "Potential transcription error"]
```

There was one compound with potential transcription errors for Ki value.

These compounds with questionable Ki values were removed, as they could be potential sources of errors for ML models later on (error trickling effect). One of the ways I thought of doing was to fill the empty cells within the "data_validity_comment" column first, so it would be easier to filter them out.

```{python}
# Fill "NaN" entries with an actual name e.g. zero
dtree_df = dtree_df.fillna("none")
dtree_df.head()
```

Filtered out only the compounds with nil data validity comments.

```{python}
#dtree_df["data_validity_comment"].unique()
dtree_df = dtree_df[dtree_df["data_validity_comment"] == "none"]
```

Checking the dtree_df dataframe again and also whether if only the compounds with "none" labelled for "data_validity_comment" column were kept (and other two types were removed).

```{python}
print(dtree_df.shape)
dtree_df["data_validity_comment"].unique()
```

<br>

#### **Sanitise compounds**

I've found the [pre-processing molecules tutorial](https://docs.datamol.io/stable/tutorials/Preprocessing.html) and its reference links provided by Datamol to be very informative. Each steps of fix_mol(), sanitize_mol() and standardize_mol() were explained in the link provided above. I guess the key was to select pre-processing options required to fit the purpose of the ML models being built later on, and more experiences in doing so would hopefully help with improving the compound pre-processing step.

```{python}
# _preprocess function to sanitise compounds - adapted from datamol.io

smiles_column = "smiles"

dm.disable_rdkit_log()

def _preprocess(row):
    # Convert each compound to a RDKit molecule in the smiles column
    mol = dm.to_mol(row[smiles_column], ordered=True)
    # Fix common errors in the molecules
    mol = dm.fix_mol(mol)
    # Sanitise the molecules 
    mol = dm.sanitize_mol(mol, sanifix=True, charge_neutral=False)
    # Standardise the molecules
    mol = dm.standardize_mol(
        mol,
        # Switch on to disconnect metal ions
        disconnect_metals=True,
        normalize=True,
        reionize=True,
        # Switch on "uncharge" to neutralise charges
        uncharge=True,
        # Taking care of stereochemistries of compounds
        stereo=True,
    )

    # Added a new column below for RDKit molecules
    row["rdkit_mol"] = dm.to_mol(mol)
    row["standard_smiles"] = dm.standardize_smiles(dm.to_smiles(mol))
    row["selfies"] = dm.to_selfies(mol)
    row["inchi"] = dm.to_inchi(mol)
    row["inchikey"] = dm.to_inchikey(mol)
    return row
```

Then the compound sanitisation function was applied to the dtree_df.

```{python}
dtree_san_df = dtree_df.apply(_preprocess, axis = 1)
dtree_san_df.head()
```

Please note if the dataset required for sanitisation is large, Datamol has suggested using their example code to add parallelisation as shown below.

``` {{python}}
# Code adapted from: https://docs.datamol.io/stable/tutorials/Preprocessing.html#references
data_clean = dm.parallelized(
    _preprocess, 
    data.iterrows(), 
    arg_type="args", 
    progress=True, 
    total=len(data)
    )
data_clean = pd.DataFrame(data_clean)
```

```{python}
dtree_san_df.shape
```

In this case, I tried using the preprocessing function without adding the parallelisation, the whole sanitisation process wasn't very long (since I had a small dataset), and was done within a minute or so.

<br>

#### **Detect outliers**

Plotting a histogram to see the distribution of pKi values first.

```{python}
dtree_san_df.hist(column = "pKi")
```

I read a bit about Dixon's Q test and realised that there were a few required assumptions prior to using this test, and the current dataset used here (dtree_san_df) might not fit the requirements, which were:

-   normally distributed data
-   a small sample size e.g. between 3 and 10, which was originally stated in this paper [@dean1951].

So I've decided that rather than showing Python code for Dixon's Q test myself, I'd attach a few examples from others instead, [Q test from Plotly](https://plotly.com/python/v3/outlier-test/) and also [Dixon's Q test for outlier identification -- a questionable practice](https://sebastianraschka.com/Articles/2014_dixon_test.html), since this dataset here wasn't quite normally distributed as shown from the histogram plotted above.

```{python}
dtree_san_df.boxplot(column = "pKi")

# the boxplot version below shows a blank background
# rather than above version with horizontal grid lines
#dtree_san_df.plot.box(column = "pKi")
```

I also used Pandas' built-in boxplot in addition to the histogram to show the possible outliers within the pKi values. Clearly, the outliers for pKi values appeared to be above 10. I've also decided not to completely remove these outliers due to the dataset itself being not in a Gaussian distribution (which meant they might not be true outliers).

<br>

#### **Calculate RDKit 2D molecular descriptors**

I've explored a few different ways to compute molecular descriptors, essentially RDKit was used as the main library to do this. A [blog post](https://drzinph.com/rdkit_2d-descriptors-in-python-part-4/) I've come across on calculating RDKit 2D molecular descriptors has explained it well, it gave details about how to bundle the functions together in a class (the idea of building a small library yourself to use in projects, which were quite handy). I've also read into RDKit's documentations and also the ones from Datamol. Rather than re-inventing the wheels as there were already several really well-explained blog posts about this, I'd probably opt for Datamol since it was relatively new to me.

With the lastest format of the dtree_san_df, it already included a RDKit molecule column (named "rdkit_mol"), so this meant I could go ahead with the calculations. Without jumping into Datamol directly, I just wanted to try out RDKit's code first. There was also a useful [descriptor calculation tutorial](https://greglandrum.github.io/rdkit-blog/posts/2022-12-23-descriptor-tutorial.html) written by Greg Landrum about this.

###### **RDKit code**

This was just one of the code examples in RDKit as there could be more variations of this depending on needs.

```{python}
# Saving the RDKit mol column as an object
mols_rdkit = dtree_san_df["rdkit_mol"]
mols_rdkit

# Run descriptor calculations on mols object
# and save as a new list
mol_ls = [Descriptors.CalcMolDescriptors(mol) for mol in mols_rdkit]

# Convert the list into a dataframe
df_rdkit_2d = pd.DataFrame(mol_ls)
print(df_rdkit_2d.shape)
df_rdkit_2d.head()
```

In total, it generated 209 descriptors. 

<br>

###### **Datamol code**

Then I tested Datamol's code on this as shown below.

```{python}
# Convert RDKit molecule column into list
mols_dm = dtree_san_df["rdkit_mol"]

# Datamol's batch descriptor code for a list of compounds
dtree_san_df_dm = dm.descriptors.batch_compute_many_descriptors(mols_dm)
print(dtree_san_df_dm.shape)
dtree_san_df_dm.head()
```

There were a total of 22 molecular descriptors generated, which seemed more like what I might use for the decision tree model. The types of descriptors were shown below.

```{python}
dtree_san_df_dm.columns
```

<br>

##### **Combine dataframes**

The trickier part for this second-half of data pre-processing actually lied here -
merging/joining/concatenating dataframes of the pre-processed dataframe (dtree_san_df) and Datamol's descriptor code (dtree_san_df_dm). 

Initially, I tried using all of Pandas' code of merge/join/concat dataframes. They all failed to create the correct final combined dataframe since there were too many rows, with one run actually created 540 rows (maximum should be 481 rows). One of the possible reasons for this could be that some of the descriptors had zeros generated as results for some of the compounds, and when combining using Pandas code like the ones mentioned here, they might cause unexpected results (as suggested by Pandas, they were not exactly equivalent to SQL joins). So I looked into different ways, and while there were no other common columns for both dataframes, the index column seemed to be the only one that correlated both.

I also found out after going back to the previous steps that when I applied the compound pre-processing function from Datamol, the index of the resultant dataframe was changed to start from 1 (rather than zero). Because of this, I tried re-setting the index of dtree_san_df first, then dropped the index column, followed by re-setting the index again to ensure it started at zero, and this worked. So now the dtree_san_df would have exactly the same index as the one for dtree_san_df_dm.

```{python}
# 1st index re-set
dtree_san_df = dtree_san_df.reset_index()
# Drop the index column
dtree_san_df = dtree_san_df.drop(["index"], axis = 1)
dtree_san_df.head()
```

```{python}
# 2nd index re-set
dtree_san_df = dtree_san_df.reset_index()
print(dtree_san_df.shape)
dtree_san_df.head()
```

Also re-setting the index of the dtree_san_df_dm.

```{python}
dtree_san_df_dm = dtree_san_df_dm.reset_index()
print(dtree_san_df_dm.shape)
dtree_san_df_dm
```

Merged both dataframes of dtree_san_df and dtree_san_df_dm based on both of their indices.

```{python}
# merge dtree_san_df & dtree_san_df_dm
dtree_f_df = pd.merge(
    dtree_san_df[["index", "molecule_chembl_id", "pKi"]],
    dtree_san_df_dm,
    left_index=True,
    right_index=True
)
```

Check final dataframe to make sure there were 481 rows (index_x and index_y were identical) and increased number of columns (combined columns from both dataframes). So this finally seemed to work.

```{python}
print(dtree_f_df.shape)
dtree_f_df.head(10)
```

TODOs:
* Remove index_x & index_y
* Add subsection: Data pre-processing reflections
- order of steps could be changed in a more logical order
- checklist of things to do during data pre-processing (refer to note)
