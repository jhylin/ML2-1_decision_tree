---
title: "Decision tree"
subtitle: "ML series 2.1 - model building"
author: Jennifer HY Lin
date: '2023-9-14'
draft: true
categories: 
    - Machine learning projects
    - Tree models
    - Pandas
    - Scikit-learn
    - ChEMBL database
jupyter: python3
format: html
bibliography: references.bib
---

##### **Estimate experimental errors**

This part was about estimating the impact of experimental errors (pKi values) on the predictive machine learning (ML) models. It was also needed to estimate the maximum possible correlation that could be drawn from the dataset used and pre-processed from the previous two posts. 

This subsection was highly inspired by Pat Walters' posts, which have discussed about estimating errors for experimental data with code links available in these posts:

- [How Good Could (Should) My Models Be?](http://practicalcheminformatics.blogspot.com/2019/07/how-good-could-should-my-models-be.html) - a reference paper [@Brown2009] was mentioned as the simulation basis for estimating the impact of experimental errors on the correlation from a predictive ML model

- [Getting Real with Molecular Property Prediction](http://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html) (under subsection of "How Well Should We Be Able to Model the Data?")

To get started, all the required libraries were loaded as below. 

```{python}
import pandas as pd
import numpy as np
from sklearn.metrics import r2_score
import seaborn as sns
```

Imported the pre-processed data from previous posts.

```{python}
# Import data
dtree = pd.read_csv("ache_2d_chembl.csv")
dtree.drop(columns = ["Unnamed: 0"], inplace=True)
dtree.head()
```

```{python}
dtree.columns
```

The pKi column was used in the code below as it contained the experimental values (calculated from measured Ki values, usually derived from countless lab experiments) collected from different scientific literatures or other sources as stated in ChEMBL. The aim was to simulate pKi values with experimental errors added to them.

*Code used for the rest of the subsection were adapted with thanks from Pat Walters' "[maximum_correlation.ipynb](https://github.com/PatWalters/maximum_correlation/blob/master/maximum_correlation.ipynb)" with my own added comments for further explanations*

```{python}
# Save exp data (pKi) as an object
data = dtree["pKi"]
# Save the object as a list
data_ls = [data]

# Trial 3-, 5- & 10-fold errors
for fold in (3, 5, 10):
    # Retrieve error samples randomly from a normal distribution
    # Bewteen 0 and log10 of number-fold 
    # for the length of provided data only
    error = np.random.normal(0, np.log10(fold), len(data))
    data_ls.append(error + data)

# Convert data_ls to dataframe
dtree_err = pd.DataFrame(data_ls)
# Re-align dataframe (switch column header & index)
dtree_err = dtree_err.transpose()
# Rename columns
dtree_err.columns = ["pKi", "3-fold", "5-fold", "10-fold"]
print(dtree_err.shape)
dtree_err.head()
```

Melting the created dtree_err so it could be plotted later (noticed there should be an increased number of rows after re-stacking the data).

```{python}
# Melt the dtree_err dataframe 
# to make error values in one column (for plotting)
dtree_err_melt = dtree_err.melt(id_vars = "pKi")
print(dtree_err_melt.shape)
dtree_err_melt.head()
```

Presenting this in regression plots.

*Note: There was a matplotlib bug which would always show a tight_layout user warning for FacetGrid plots in seaborn (the lmplot used below). Seaborn was built based on matplotlib so unsurprisingly this occurred (this [GitHub issue link](https://github.com/matplotlib/matplotlib/issues/26290) might explain it). I have therefore temporarily silenced this user warning for the sake of post publication.*

```{python}
# To silence the tight-layout user warning
import warnings
warnings.filterwarnings("ignore")

# variable = error-fold e.g. 3-fold
# value = pKi value plus error
sns.set_theme(font_scale = 1.5)
plot = sns.lmplot(
    x = "pKi", 
    y = "value", 
    col = "variable", 
    data = dtree_err_melt, 
    # alpha = markâ€™s opacity (low - more transparent)
    # s = mark size (increase with higher number)
    scatter_kws = dict(alpha = 0.5, s = 15)
    )
title_list = ["3-fold", "5-fold", "10-fold"]
for i in range(0, 3):
    plot.axes[0, i].set_ylabel("pKi + error")
    plot.axes[0, i].set_title(title_list[i])
```

Simulating the impact of error on the correlation between experimental pKi and also pKi with errors (3-fold, 5-fold and 10-fold). R^2^ calculated using [*scikit-learn*](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score) was introduced in the code below.

```{python}
# Calculating r2 score (coefficient of determination) 
# based on 1000 trials for each fold
# note: data = dtree["pKi"]

# Create an empty list for correlation
cor_ls = []
for fold in [3, 5, 10]:
    # Set up 1000 trials
    for i in range(0, 1000):
        error = np.random.normal(0, np.log10(fold), len(data))
        cor_ls.append([r2_score(data, data + error), f"{fold}-fold"])

# Convert cor_ls into dataframe
err_df = pd.DataFrame(cor_ls, columns = ["r2", "fold_error"])
err_df.head()
```

Plotting the R^2^ and fold-errors as violin plots.

```{python}
sns.set_theme(rc = {"figure.figsize": (9, 8)}, font_scale = 1.5)
vplot = sns.violinplot(x = "fold_error", y = "r2", data = err_df)
vplot.set(xlabel = "Fold error", ylabel = "R$^2$")
```

This definitely helped a lot with visualising the estimated errors for the experimental Ki values curated in ChEMBL for this specific protein target (CHEMBL220, acetylcholinesterase). The larger the error-fold, the lower the R^2^, and once the experimental error reached 10-fold, we could see an estimated R^2^ (maximum correlation) with its median sitting below 0.55, indicating a likely poor predictive ML model if it was built based on these data with the estimated experimental errors.

<br>

##### **Check max phase distribution**

At this stage, I've planned to do training/testing split on compounds with max phase of 4 (i.e. they were usually prescription medicines used in real-life, but I had a feeling this might further reduce the sample size even further).

Max phases were assigned to each ChEMBL-curated compound according to this [ChEMBL FAQ link](https://chembl.gitbook.io/chembl-interface-documentation/frequently-asked-questions/drug-and-compound-questions) (under the question of "What is max phase?"). Max phase 4 meant the compound was an approved and marketed drug (e.g. prescription medicines), and the lower the number, the more likely the compounds were still in different phases of clinical trials or pending to be entered into clinical trials.

Checking out the distribution of max phase assignments for all compounds in this collected and pre-processed dataset first. 

```{python}
dtree["max_phase"].describe()
```

It was actually a very small number of compounds that were assigned with a number for max phase (total count of 10, which was unsurprising as there weren't many acetylcholinesterase inhibitors being used as prescription medications for dementia, with some well-known examples such as donepezil, galantamine and rivastigmine).

Filling in actual "null" labels for all "NaN" rows in the "max_phase" columns.

```{python}
dtree["max_phase"].fillna("null", inplace=True)
dtree.head()
```

Checking out the actual counts of each max phase category.

```{python}
dtree[["molecule_chembl_id", "max_phase"]].groupby("max_phase").count()
```

Unfortunately this has reduced the dataset even further if I opted to go for training the model on compounds with max phase of 4 (there were literally only 5!). Because of this, I think I'd build a model using all of these 442 compounds first (that would be already a small sample too, but better than only 5) and see what I would get. This might also hint at using an ensemble of tree models instead (planned for future posts), as it's more accurate and powerful averaging the results from multiple tree models than only from a single tree model.

<br>

##### **Sanity check on dtree df**

Another sanity check on dtree dataframe - making sure there weren't any "NaN" cells in it!

```{python}
dtree.dropna()
print(dtree.shape)
dtree.head()
```

<br>

##### **Model building**

*work-in-progress*

Setting up decision tree model using scikit-learn's tree.DecisionTreeClassifier() - this was on the full dataset of 442 compounds.

```{python}
dtree.columns
```

```{python}
# Define features/predictors
X = dtree[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]

X
```

```{python}
# Define target/outcome
y = dtree['pKi']
y
```

```{python}
# Convert X (features/predictors) to numpy array
X = X.to_numpy()

# Convert y (outcome/target)
y = y.to_numpy()

print(type(X))
print(type(y))
```
```{python}
# Check X is now an array
X
```

```{python}
y
```

```{python}
from sklearn import tree
# May need to reduce max_depth
ache_tree_1 = tree.DecisionTreeRegressor(max_depth=2).fit(X, y)
ache_tree_2 = tree.DecisionTreeRegressor(max_depth=4).fit(X, y)
```

```{python}
tree.plot_tree(ache_tree_1)
```

```{python}
tree.plot_tree(ache_tree_2)
```

<br>

###### **Splitting dtree dataframe based on max phase**

*Testing in progress*

Fitting DecisionTreeRegressor() on X, y as dtree df with max phase of 4 (5 compounds) - meaning X_test will likely need to be this same sample size as well.

```{python}
dtree.head()
```

```{python}
# Compounds with max phase of 4
dtree_mp4 = dtree[dtree["max_phase"] == 4]
dtree_mp4
```

```{python}
# Set X (features) for max phase 4 compounds
X_mp4 = dtree_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]
X_mp4
```

```{python}
# Convert X_mp4 to numpy array
X_mp4 = X_mp4.to_numpy()
X_mp4
```

```{python}
y_mp4 = dtree_mp4["pKi"]
y_mp4 = y_mp4.to_numpy()
y_mp4
```

```{python}
ache_tree_mp4_1 = tree.DecisionTreeRegressor(max_depth=2)
ache_tree_mp4_1.fit(X_mp4, y_mp4)

ache_tree_mp4_2 = tree.DecisionTreeRegressor(max_depth=5)
ache_tree_mp4_2.fit(X_mp4, y_mp4)
```

```{python}
tree.plot_tree(ache_tree_mp4_1)
```

```{python}
tree.plot_tree(ache_tree_mp4_2)
```

```{python}
# Compounds with max phase other than 4 (null to 3)
# X_test
dtree_mp_null = dtree[dtree["max_phase"] == "null"]
print(dtree_mp_null.shape)
dtree_mp_null.head() 
```

There were 432 compounds with max phase as "null".

TODOs:
- ?quick EDA on these "null" compounds to see distribution of pKi.
- random sampling

```{python}
# Randomly sample 5 compounds from dtree_mp_null - pandas.DataFrame.sample


```

```{python}
X_mp_test = dtree_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]
X_mp_test
```

```{python}
# Convert dtree_mp to numpy array
X_mp_test = X_mp_test.to_numpy()
X_mp_test
```

```{python}
y_1 = ache_tree_mp4_1.predict(X_mp_test)
y_2 = ache_tree_mp4_2.predict(X_mp_test)
```

```{python}
import matplotlib.pyplot as plt

# plt.figure()
# plt.scatter(X_mp4, y_mp4, s=20, edgecolors="black", c="darkred", label="data")

# plt.plot(X_mp_test, y_1, color="cornflowerblue", label="max_depth=2", linewidth = 2)

# plt.plot(X_mp_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)

# plt.xlabel("data")
# plt.ylabel("target")
# plt.title("Decision tree regression for AChE inhibitors from ChEMBL")
# plt.legend()
# plt.show()
```


