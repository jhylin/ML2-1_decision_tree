---
title: "Tree models in ChEMBL data"
subtitle: "ML series 2.1 - decision tree - model building"
author: Jennifer HY Lin
date: '2023-9-12'
draft: true
categories: 
    - Machine learning projects
    - Pandas
    - Scikit-learn
    - ChEMBL database
jupyter: python3
format: html
bibliography: references.bib
---

##### **Estimate experimental errors**

This part was about estimating the impact of experimental errors (pKi values) on predictive machine learning (ML) models. It was also needed to estimate the maximum possible correlation that could be drawn with the dataset used and pre-processed from the previous two posts. 

This subsection was highly inspired by P. Walters posts, which have discussed about estimating errors for experimental data with code links available in these posts:

- [How Good Could (Should) My Models Be?](http://practicalcheminformatics.blogspot.com/2019/07/how-good-could-should-my-models-be.html) *reference paper*

- [Getting Real with Molecular Property Prediction](http://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html) (under subsection of "How Well Should We Be Able to Model the Data?")

To get started, all the requried libraries were loaded as below.

```{python}
import pandas as pd
import numpy as np
import math
from sklearn.metrics import r2_score
import seaborn as sb
```

Imported the pre-process data from previous post.

```{python}
# Import data
dtree = pd.read_csv("ache_2d_chembl.csv")
dtree.drop(columns = ["Unnamed: 0"], inplace=True)
dtree.head()
```

The pKi column was being used in the code below, which were usually derived from lab experiments collected across different scientific literatures or other sources as stated in ChEMBL. The aim was to simulate pKi values with experimental errors added to them.

*Code used for the rest of the subsection were adapted with thanks from P. Walters' "[maximum_correlation.ipynb](https://github.com/PatWalters/maximum_correlation/blob/master/maximum_correlation.ipynb)"*

```{python}
# Save exp data (pKi) as an object
data = dtree["pKi"]
# Save the object as a list
data_ls = [data]

# Trial 3-, 5- & 10-fold errors
for fold in (3, 5, 10):
    # Retrieve error samples randomly from a normal distribution
    # Bewteen 0 and log10 of number-fold 
    # for the length of provided data only
    error = np.random.normal(0, np.log10(fold), len(data))
    data_ls.append(error + data)

# Convert data_ls to df
dtree_err = pd.DataFrame(data_ls)
# Re-align dataframe (switch column header & index)
dtree_err = dtree_err.transpose()
# Rename columns
dtree_err.columns = ["pKi", "3-fold", "5-fold", "10-fold"]
print(dtree_err.shape)
dtree_err.head()
```

Melting the created dtree_err so it could be plotted later.

```{python}
# Melt the dtree_err dataframe 
# to make error values in one column (for plotting)
dtree_err_melt = dtree_err.melt(id_vars = "pKi")
print(dtree_err_melt.shape)
dtree_err_melt.head()
```

Presenting this in regression plots.

```{python}
# variable = error-fold e.g. 3-fold
# value = pKi value plus error

sb.set_theme(font_scale = 1.5)
plot = sb.lmplot(
    x = "pKi", 
    y = "value", 
    col = "variable", 
    data = dtree_err_melt, 
    scatter_kws = dict(alpha = 0.5, s = 15)
    )
title_list = ["3-fold", "5-fold", "10-fold"]
for i in range(0, 3):
    plot.axes[0, i].set_ylabel("pKi + error")
    plot.axes[0, i].set_title(title_list[i])
```

Simulating the impact of error on the correlation between experimental pKi and also pKi with errors (3-fold, 5-fold and 10-fold). R^2^ calculated using [*scikit-learn*](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score) was introduced in the code below.

```{python}
# Calculating r2 score (coefficient of determination) 
# based on 1000 trials for each fold
# note: data = dtree["pKi"]

# Create an empty list for correlation
cor_ls = []
for fold in [3, 5, 10]:
    # Set up 1000 trials
    for i in range(0, 1000):
        error = np.random.normal(0, np.log10(fold), len(data))
        cor_ls.append([r2_score(data, data + error), f"{fold}-fold"])

# Convert cor_ls into dataframe
err_df = pd.DataFrame(cor_ls, columns = ["r2", "fold_error"])
err_df.head()
```

Plotting the R^2^ and fold-errors as violin plots.

```{python}
sb.set_theme(rc = {"figure.figsize": (9, 8)}, font_scale = 1.5)
bplot = sb.violinplot(x = "fold_error", y = "r2", data = err_df)
bplot.set(xlabel = "Fold error", ylabel = "R$^2$")
```

This definitely helped a lot in visualising the estimated errors in the experimental pKi curated in ChEMBL for this specific protein target (CHEMBL220), an acetylcholinesterase. The larger the error-fold, the lower the R^2^, and once the experimental error reached 10-fold, we could see an estimated R^2^ (maximum correlation) sitting below 0.55.

<br>

##### **Model building**

Set up decision tree model using scikit-learn's tree.DecisionTreeClassifier() or tree.DecisionTreeRegressor() (for regression problems)